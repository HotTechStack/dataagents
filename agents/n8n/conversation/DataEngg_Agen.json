{
  "name": "DataEngg-Agent",
  "nodes": [
    {
      "parameters": {
        "httpMethod": "POST",
        "path": "data-engineering-agent",
        "responseMode": "responseNode",
        "options": {
          "responseHeaders": {
            "entries": [
              {
                "name": "Access-Control-Allow-Origin",
                "value": "*"
              }
            ]
          }
        }
      },
      "id": "5453af0e-b64d-4350-84ac-8850b1348cf0",
      "name": "Webhook",
      "type": "n8n-nodes-base.webhook",
      "typeVersion": 1,
      "position": [
        -1120,
        480
      ],
      "webhookId": "87f6c8c3-55de-4661-8fd9-2727490b076f"
    },
    {
      "parameters": {
        "language": "python",
        "pythonCode": "import json\nfrom datetime import datetime\n\n# Get all input items\nall_inputs = _input.all()\n\n# For debugging\nprint(\"DEBUG: All input items:\")\nprint(json.dumps(all_inputs, indent=2, default=str))\n\n# Process each item\nfor item in all_inputs:\n    # Try to get the webhook data from the item\n    data = item.json if hasattr(item, \"json\") else {}\n    \n    # Check if this is our webhook data\n    if \"selectedAgentIds\" in data and \"selectedStrategy\" in data and \"query\" in data:\n        # Extract the required fields\n        selected_agent_ids = data.get('selectedAgentIds', [])\n        selected_strategy = data.get('selectedStrategy', '')\n        query = data.get('query', '')\n\n        # Validate input\n        if not selected_agent_ids or not isinstance(selected_agent_ids, list) or len(selected_agent_ids) == 0:\n            item.json = {\n                'success': False,\n                'error': 'No agents selected',\n                'statusCode': 400\n            }\n            continue\n\n        if not query:\n            item.json = {\n                'success': False,\n                'error': 'Query is required',\n                'statusCode': 400\n            }\n            continue\n\n        # Map agent IDs to descriptions for context\n        agent_context = {\n            'data-architect': 'Expert in designing data infrastructure and systems',\n            'pipeline-engineer': 'Expert in building efficient data pipelines',\n            'data-analyst': 'Expert in analyzing and interpreting complex data',\n            'data-scientist': 'Expert in applying statistical models and machine learning',\n            'data-governance': 'Expert in ensuring data quality and compliance',\n            'data-engineer': 'Expert in building and maintaining data infrastructure'\n        }\n\n        # Build context for selected agents\n        selected_agents = [{\n            'id': agent_id,\n            'context': agent_context.get(agent_id, f\"Expert in {agent_id.replace('-', ' ')}\")\n        } for agent_id in selected_agent_ids]\n\n        # Get strategy description\n        strategy_description = 'Sequential conversation where each agent responds in turn.'\n        if selected_strategy == 'collaborative':\n            strategy_description = 'Collaborative approach where agents work together on the solution.'\n        elif selected_strategy == 'debate':\n            strategy_description = 'Debate format where agents may present different perspectives.'\n\n        # Update the item's json with our processed data\n        item.json = {\n            'selectedAgents': selected_agents,\n            'query': query,\n            'strategyDescription': strategy_description,\n            'timestamp': datetime.now().isoformat(),\n        }\n\n# Return all processed items\nreturn all_inputs"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -820,
        480
      ],
      "id": "5532c320-992d-417a-ad0e-e0df2fa30606",
      "name": "Process Agent Request"
    },
    {
      "parameters": {
        "promptType": "define",
        "text": "=You are a multi-agent data engineering system that simulates multiple expert agents collaborating together. Based on the selected agents, you'll provide comprehensive solutions that blend expertise from all selected specialists.\n\nCurrently, you are acting as these experts:\n{{ $('Process Agent Request').item.json.body.selectedAgentIds }}\n\nFor your response:\n1. Structure your answer to clearly show which expert is providing which part of the solution\n2. Include practical implementation details and examples where appropriate\n3. Address scalability, reliability, and performance considerations\n4. If relevant, include sample architecture diagrams described in text format\n5. Provide configurations or flow diagram when they would be helpful\n\nRemember: Focus on providing actionable advice that directly addresses the query without any unnecessary introductions.\n\nUser Query: {{$node[\"Webhook\"].json.body.query}}\n\n",
        "messages": {
          "messageValues": [
            {
              "message": "=To maintain context and fully understand the user's question, always review the previous conversation between you and him before providing an answer.\nThis is the previous conversation:\n{{ $('Aggregate').item.json[\"context\"].map(m => `\nHuman: ${m.human || 'undefined'}\nAI Assistant: ${m.ai || 'undefined'}\n`).join('') }}"
            },
            {
              "type": "HumanMessagePromptTemplate",
              "message": "** Please generate your response strictly in valid JSON format with exactly two keys: \"agentResponses\" and \"summary\". The \"agentResponses\" key should map to an array of objects. Each object must include exactly three keys: \"agentId\" (a string), \"agentName\" (a string), and \"content\" (a string containing the detailed response). The \"summary\" key should map to a string that briefly integrates all the perspectives from the \"agentResponses\". Do not include any additional text, explanations, or markdown formatting. Your entire output should be valid JSON that can be parsed directly by Python's json module.**"
            }
          ]
        }
      },
      "type": "@n8n/n8n-nodes-langchain.chainLlm",
      "typeVersion": 1.5,
      "position": [
        460,
        80
      ],
      "id": "3e6d07cf-4c26-4f7f-a047-05743d2dcc42",
      "name": "Basic LLM Chain"
    },
    {
      "parameters": {
        "model": {
          "__rl": true,
          "value": "o3-mini",
          "mode": "list",
          "cachedResultName": "o3-mini"
        },
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.lmChatOpenAi",
      "typeVersion": 1.2,
      "position": [
        500,
        660
      ],
      "id": "6d639fbb-f7d4-48f5-aa89-65bddd403954",
      "name": "OpenAI Chat Model",
      "credentials": {
        "openAiApi": {
          "id": "9D4qN5h10zXTHdGb",
          "name": "OpenAi account"
        }
      }
    },
    {
      "parameters": {
        "language": "python",
        "pythonCode": "import json\n\nall_inputs = _input.all()\noutput = []\n\nfor item in all_inputs:\n    # Get the JSON data from the item\n    data = item.json if hasattr(item, \"json\") else {}\n    \n    # If the data is coming as a string in the \"text\" field, decode it.\n    if \"text\" in data:\n        try:\n            data = json.loads(data[\"text\"])\n        except Exception as e:\n            raise Exception(\"Failed to parse JSON from the text field: \" + str(e))\n    \n    # Extract agentResponses and summary\n    agent_responses = data.get(\"agentResponses\", [])\n    summary = data.get(\"summary\", \"\")\n    \n    # Create an output item for each agent response\n    for agent in agent_responses:\n        output.append({\n            \"json\": {\n                \"agentId\": agent.get(\"agentId\", \"\"),\n                \"agentName\": agent.get(\"agentName\", \"\"),\n                \"content\": agent.get(\"content\", \"\")\n            }\n        })\n    \n    # Create a separate output item for the summary\n    output.append({\n        \"json\": {\n            \"summary\": summary\n        }\n    })\n\nreturn output"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        920,
        80
      ],
      "id": "3d187f6b-7a96-4670-baea-c70318cd5ff1",
      "name": "Code"
    },
    {
      "parameters": {
        "respondWith": "allIncomingItems",
        "options": {}
      },
      "type": "n8n-nodes-base.respondToWebhook",
      "typeVersion": 1.1,
      "position": [
        1460,
        400
      ],
      "id": "20959e8a-f6f2-4aff-958e-ab3558f9ea87",
      "name": "Respond to Webhook"
    },
    {
      "parameters": {
        "aggregate": "aggregateAllItemData",
        "destinationFieldName": "context",
        "options": {}
      },
      "id": "54e8fc4f-d863-4fdf-9436-30054bc1d923",
      "name": "Aggregate",
      "type": "n8n-nodes-base.aggregate",
      "position": [
        140,
        0
      ],
      "typeVersion": 1,
      "alwaysOutputData": true
    },
    {
      "parameters": {
        "content": "Get Context",
        "height": 237,
        "width": 575,
        "color": 7
      },
      "id": "886c4cd5-2ee6-49ae-b928-0f8aa037450a",
      "name": "Sticky Note",
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        -300,
        -80
      ],
      "typeVersion": 1
    },
    {
      "parameters": {
        "options": {}
      },
      "id": "f89e06c6-004d-4d6c-8777-57939c4636c9",
      "name": "Chat Memory Manager",
      "type": "@n8n/n8n-nodes-langchain.memoryManager",
      "position": [
        -240,
        0
      ],
      "typeVersion": 1,
      "alwaysOutputData": true
    },
    {
      "parameters": {
        "sessionIdType": "customKey",
        "sessionKey": "data_agent_conversation_X112n997y"
      },
      "type": "@n8n/n8n-nodes-langchain.memoryBufferWindow",
      "typeVersion": 1.3,
      "position": [
        -60,
        720
      ],
      "id": "2fc0b00f-b8dc-4449-8c98-cd51b8f81903",
      "name": "Window Buffer Memory"
    },
    {
      "parameters": {
        "public": true,
        "options": {
          "loadPreviousSession": "memory"
        }
      },
      "type": "@n8n/n8n-nodes-langchain.chatTrigger",
      "typeVersion": 1.1,
      "position": [
        -720,
        80
      ],
      "id": "7d2fff47-575e-49f1-985f-408308d1dbea",
      "name": "Chat Trigger",
      "webhookId": "b2f479b7-83af-4ddf-a494-621127b5f96c"
    },
    {
      "parameters": {
        "mode": "insert",
        "messages": {
          "messageValues": [
            {
              "type": "user",
              "message": "={{ $('Basic LLM Chain').item.json.text }}"
            },
            {
              "type": "ai",
              "message": "={{ $('Basic LLM Chain').item.json.text }}"
            }
          ]
        }
      },
      "id": "77a2b9fe-b4b3-4727-bf4b-1b983360bc28",
      "name": "Insert Chat",
      "type": "@n8n/n8n-nodes-langchain.memoryManager",
      "position": [
        1320,
        80
      ],
      "typeVersion": 1,
      "alwaysOutputData": true
    },
    {
      "parameters": {
        "content": "## Save Context",
        "height": 231.05945912581728,
        "width": 321.2536584847704,
        "color": 6
      },
      "id": "e07dc923-122f-49c8-b5bc-1ec2636f91b1",
      "name": "Sticky Note1",
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        1280,
        20
      ],
      "typeVersion": 1
    },
    {
      "parameters": {
        "content": "### The \"Get Chat,\" \"Insert Chat,\" and \"Window Buffer Memory\" nodes will help the LLM model maintain context throughout the conversation.",
        "height": 91.01435855269375,
        "width": 487.4293487597613,
        "color": 6
      },
      "id": "08230c9f-961d-428c-8172-aabfe9c8ac9d",
      "name": "Sticky Note2",
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        -300,
        -180
      ],
      "typeVersion": 1
    }
  ],
  "pinData": {
    "Webhook": [
      {
        "json": {
          "headers": {
            "host": "localhost:5678",
            "user-agent": "curl/8.7.1",
            "accept": "*/*",
            "content-type": "application/json",
            "content-length": "186"
          },
          "params": {},
          "query": {},
          "body": {
            "selectedAgentIds": [
              "data-architect",
              "data-engineer"
            ],
            "selectedStrategy": "collaborative",
            "query": "How to design a scalable data pipeline for real-time analytics?"
          },
          "webhookUrl": "http://localhost:5678/webhook/data-engineering-agent",
          "executionMode": "production"
        }
      }
    ]
  },
  "connections": {
    "Webhook": {
      "main": [
        [
          {
            "node": "Process Agent Request",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Process Agent Request": {
      "main": [
        [
          {
            "node": "Chat Memory Manager",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "OpenAI Chat Model": {
      "ai_languageModel": [
        [
          {
            "node": "Basic LLM Chain",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    },
    "Basic LLM Chain": {
      "main": [
        [
          {
            "node": "Code",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Code": {
      "main": [
        [
          {
            "node": "Respond to Webhook",
            "type": "main",
            "index": 0
          },
          {
            "node": "Insert Chat",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Chat Memory Manager": {
      "main": [
        [
          {
            "node": "Aggregate",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Aggregate": {
      "main": [
        [
          {
            "node": "Basic LLM Chain",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Window Buffer Memory": {
      "ai_memory": [
        [
          {
            "node": "Chat Memory Manager",
            "type": "ai_memory",
            "index": 0
          },
          {
            "node": "Chat Trigger",
            "type": "ai_memory",
            "index": 0
          },
          {
            "node": "Insert Chat",
            "type": "ai_memory",
            "index": 0
          }
        ]
      ]
    },
    "Chat Trigger": {
      "main": [
        [
          {
            "node": "Chat Memory Manager",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Insert Chat": {
      "main": [
        []
      ]
    }
  },
  "active": true,
  "settings": {
    "executionOrder": "v1",
    "timezone": "Europe/Berlin",
    "callerPolicy": "workflowsFromSameOwner"
  },
  "versionId": "c1f9794f-7422-4f60-8f4d-11589f409b38",
  "meta": {
    "templateCredsSetupCompleted": true,
    "instanceId": "6ed7c8c790e9be279bae1f055fa2b85bec988e25017a26c1b6cc75cae7133994"
  },
  "id": "Pf71TMDJsgqa4O72",
  "tags": [
    {
      "createdAt": "2025-03-23T09:50:13.877Z",
      "updatedAt": "2025-03-23T09:50:13.877Z",
      "id": "uSwVXc72coleYI9d",
      "name": "dataengg"
    }
  ]
}