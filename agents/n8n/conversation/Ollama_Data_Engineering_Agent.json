{
  "name": "Ollama Data Engineering Agent",
  "nodes": [
    {
      "parameters": {
        "language": "python",
        "pythonCode": "import json\nfrom datetime import datetime\n\n# Define roles and responsibilities for each agent\nagent_roles = {\n    \"data-architect\": \"\"\"As a **Data Architect**, I specialize in:\n- Designing scalable and robust data infrastructure\n- Defining data models, integration patterns, and storage systems\n- Ensuring architectural decisions support performance, cost-efficiency, and compliance\nIf the question involves infrastructure planning or data architecture decisions, I can help. Otherwise, I’ll defer to another expert.\"\"\",\n\n    \"pipeline-engineer\": \"\"\"As a **Pipeline Engineer**, I specialize in:\n- Building scalable, efficient, and fault-tolerant data pipelines (batch & streaming)\n- Orchestrating jobs using tools like Airflow, Dagster, or Prefect\n- Ensuring data delivery with retry logic and checkpointing\nIf it’s about data pipelines or orchestration, I’ve got you. Else, another agent may be more qualified.\"\"\",\n\n    \"data-analyst\": \"\"\"As a **Data Analyst**, I specialize in:\n- Exploring datasets and providing insights using SQL or BI tools\n- Creating dashboards and visual reports\n- Communicating patterns, anomalies, and business insights\nIf it's deep engineering or modeling, I’ll defer to others.\"\"\",\n\n    \"data-scientist\": \"\"\"As a **Data Scientist**, I specialize in:\n- Applying statistical analysis and ML models to solve business problems\n- Feature engineering and model deployment\n- Experimentation and A/B testing\nIf it’s about infrastructure or compliance, other agents are better equipped.\"\"\",\n\n    \"data-governance\": \"\"\"As a **Governance Specialist**, I specialize in:\n- Defining data policies, access controls, and quality frameworks\n- Ensuring regulatory compliance (e.g., GDPR, HIPAA)\n- Monitoring data lineage and audit trails\nIf it’s modeling or infra-specific, I may defer to other agents.\"\"\",\n\n    \"data-engineer\": \"\"\"As a **Data Engineer**, I specialize in:\n- Building and maintaining data ingestion frameworks\n- Working with tools like Spark, Flink, Kafka, and modern lakehouses\n- Ensuring reliable ETL/ELT flows\nIf it’s about statistical modeling or governance, another agent might help better.\"\"\"\n}\n\n# Process each input item\nresults = []\n\nfor item in _input.all():\n    selected_ids = item.json.get(\"body\", {}).get(\"selectedAgentIds\", [])\n    query = item.json.get(\"body\", {}).get(\"query\", \"\")\n\n    role_descriptions = \"\\n\\n\".join([agent_roles.get(aid, f\"Unknown agent: {aid}\") for aid in selected_ids])\n\n    prompt = f\"\"\"You are a multi-agent data engineering system that simulates multiple expert agents collaborating together. Based on the selected agents, you'll provide comprehensive solutions that blend expertise from all selected specialists.\n\nCurrently, you are acting as these experts:\n{role_descriptions}\n\nFor your response:\n1. Structure your answer to clearly show which expert is providing which part of the solution\n2. Include practical implementation details and examples where appropriate\n3. Address scalability, reliability, and performance considerations\n4. If relevant, include sample architecture diagrams described in text format\n5. Provide configurations or flow diagram when they would be helpful\n\nRemember: Focus on providing actionable advice that directly addresses the query without any unnecessary introductions.\n\nUser Query: {query}\n\"\"\"\n\n    item.json[\"prompt\"] = prompt\n    results.append(item)\n\nreturn results"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -200,
        640
      ],
      "id": "b085cee9-4476-4a76-9f33-250e329602e0",
      "name": "Process Agent Request"
    },
    {
      "parameters": {
        "promptType": "define",
        "text": "={{ $('Process Agent Request').item.json.prompt }}",
        "hasOutputParser": true,
        "messages": {
          "messageValues": [
            {
              "type": "HumanMessagePromptTemplate",
              "message": "** Please generate your response strictly in valid JSON format with exactly two keys: \"agentResponses\" and \"summary\". The \"agentResponses\" key should map to an array of objects. Each object must include exactly three keys: \"agentId\" (a string), \"agentName\" (a string), and \"content\" (a string containing the detailed response). The \"summary\" key should map to a string that briefly integrates all the perspectives from the \"agentResponses\". Do not include any additional text, explanations, or markdown formatting. Your entire output should be valid JSON that can be parsed directly by Python's json module.**\n\n**DO NOT Provide think tag as output**"
            }
          ]
        }
      },
      "type": "@n8n/n8n-nodes-langchain.chainLlm",
      "typeVersion": 1.5,
      "position": [
        860,
        320
      ],
      "id": "2e0818ba-717e-4586-ae3b-8b5eb4e75322",
      "name": "Basic LLM Chain"
    },
    {
      "parameters": {
        "respondWith": "allIncomingItems",
        "options": {}
      },
      "type": "n8n-nodes-base.respondToWebhook",
      "typeVersion": 1.1,
      "position": [
        1760,
        580
      ],
      "id": "7fa73152-910b-43d4-b322-c6f22989de62",
      "name": "Respond to Webhook"
    },
    {
      "parameters": {
        "aggregate": "aggregateAllItemData",
        "destinationFieldName": "context",
        "options": {}
      },
      "id": "eba61762-fc91-4ca1-8490-41d990131eda",
      "name": "Aggregate",
      "type": "n8n-nodes-base.aggregate",
      "position": [
        440,
        180
      ],
      "typeVersion": 1,
      "alwaysOutputData": true
    },
    {
      "parameters": {
        "content": "Get Context",
        "height": 237,
        "width": 575,
        "color": 7
      },
      "id": "62662be8-5cb5-40b7-824d-1ae9a4268461",
      "name": "Sticky Note",
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        0,
        100
      ],
      "typeVersion": 1
    },
    {
      "parameters": {
        "options": {}
      },
      "id": "5cf4603d-a514-4c8d-a495-beef113943d9",
      "name": "Chat Memory Manager",
      "type": "@n8n/n8n-nodes-langchain.memoryManager",
      "position": [
        60,
        180
      ],
      "typeVersion": 1,
      "alwaysOutputData": true
    },
    {
      "parameters": {
        "sessionIdType": "customKey",
        "sessionKey": "data_agent_ollama_X112n95",
        "contextWindowLength": 3
      },
      "type": "@n8n/n8n-nodes-langchain.memoryBufferWindow",
      "typeVersion": 1.3,
      "position": [
        240,
        900
      ],
      "id": "622551e3-2b67-4146-aa36-73d07b197774",
      "name": "Window Buffer Memory"
    },
    {
      "parameters": {
        "public": true,
        "options": {
          "loadPreviousSession": "memory"
        }
      },
      "type": "@n8n/n8n-nodes-langchain.chatTrigger",
      "typeVersion": 1.1,
      "position": [
        -420,
        260
      ],
      "id": "9b389f89-52d6-4562-8c82-95e99fbe0fd9",
      "name": "Chat Trigger",
      "webhookId": "b2f479b7-83af-4ddf-a494-621127b5f96c"
    },
    {
      "parameters": {
        "mode": "insert",
        "messages": {
          "messageValues": [
            {
              "type": "user",
              "message": "={{ $('OllamaWebhook').item.json.query }}"
            },
            {
              "type": "ai",
              "message": "={{ $json.output.last().content }}"
            }
          ]
        }
      },
      "id": "02025a9e-c668-40f2-ac79-13cd7d0d1206",
      "name": "Insert Chat",
      "type": "@n8n/n8n-nodes-langchain.memoryManager",
      "position": [
        1600,
        260
      ],
      "typeVersion": 1,
      "alwaysOutputData": true
    },
    {
      "parameters": {
        "content": "## Save Context",
        "height": 231.05945912581728,
        "width": 321.2536584847704,
        "color": 6
      },
      "id": "6597a14b-193e-469b-ae9c-3b28159f569e",
      "name": "Sticky Note1",
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        1580,
        200
      ],
      "typeVersion": 1
    },
    {
      "parameters": {
        "content": "### The \"Get Chat,\" \"Insert Chat,\" and \"Window Buffer Memory\" nodes will help the LLM model maintain context throughout the conversation.",
        "height": 91.01435855269375,
        "width": 487.4293487597613,
        "color": 6
      },
      "id": "800f6687-9f88-4387-aa9f-b2f071fedf61",
      "name": "Sticky Note2",
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        0,
        0
      ],
      "typeVersion": 1
    },
    {
      "parameters": {
        "model": "deepseek-r1:latest",
        "options": {
          "numCtx": 12192
        }
      },
      "type": "@n8n/n8n-nodes-langchain.lmChatOllama",
      "typeVersion": 1,
      "position": [
        820,
        860
      ],
      "id": "e601c542-376e-4131-ab21-f16839939764",
      "name": "Ollama Chat Model",
      "credentials": {
        "ollamaApi": {
          "id": "QfM7x1DXHJIhUPTi",
          "name": "Ollama account"
        }
      }
    },
    {
      "parameters": {
        "httpMethod": "POST",
        "path": "ollama-de-agent",
        "responseMode": "responseNode",
        "options": {
          "responseHeaders": {
            "entries": [
              {
                "name": "Access-Control-Allow-Origin",
                "value": "*"
              }
            ]
          }
        }
      },
      "id": "87c349ed-866c-4a86-bf97-f9d297c7f054",
      "name": "OllamaWebhook",
      "type": "n8n-nodes-base.webhook",
      "typeVersion": 1,
      "position": [
        -440,
        640
      ],
      "webhookId": "87f6c8c3-55de-4661-8fd9-2727490b076f"
    },
    {
      "parameters": {
        "jsonSchemaExample": "[{\n  \"summary\": \n  \"Summary of All agents output\"\n},\n{\n\"agentId\": \n\"da_001\",\n\"agentName\": \n\"Data Architect\",\n\"content\": \n\"I stated 'Design robust modular data architecture' to emphasize the importance of building a flexible and scalable infrastructure. This approach ensures that as data volumes and complexity grow, the system remains maintainable, cost-efficient, and compliant with regulatory standards by modularizing components.\"\n},\n{\n\"agentId\": \n\"pe_001\",\n\"agentName\": \n\"Pipeline Engineer\",\n\"content\": \n\"The recommendation to 'Build efficient pipelines; ensure reliability' was made to highlight the need for robust data orchestration. Efficient pipelines guarantee fault tolerance and seamless data flow through proper retry logic, job scheduling, and orchestration tools for both batch and streaming processes.\"\n}]"
      },
      "type": "@n8n/n8n-nodes-langchain.outputParserStructured",
      "typeVersion": 1.2,
      "position": [
        1520,
        1020
      ],
      "id": "c467384d-9662-411c-9cf5-f9d9a2336c35",
      "name": "Structured Output Parser"
    },
    {
      "parameters": {
        "model": "deepseek-r1:latest",
        "options": {
          "numCtx": 8192
        }
      },
      "type": "@n8n/n8n-nodes-langchain.lmChatOllama",
      "typeVersion": 1,
      "position": [
        1200,
        1000
      ],
      "id": "7dfe0d9b-d8c9-4862-83d8-7ee4ee038302",
      "name": "Ollama Chat Model1",
      "credentials": {
        "ollamaApi": {
          "id": "QfM7x1DXHJIhUPTi",
          "name": "Ollama account"
        }
      }
    },
    {
      "parameters": {
        "options": {
          "prompt": "Instructions:\n--------------\n{instructions}\n--------------\nCompletion:\n--------------\n{completion}\n--------------\n\nAbove, the Completion did not satisfy the constraints given in the Instructions.\nError:\n--------------\n{error}\n--------------\n\nPlease try again. Please only respond with an answer that satisfies the constraints laid out in the Instructions:"
        }
      },
      "type": "@n8n/n8n-nodes-langchain.outputParserAutofixing",
      "typeVersion": 1,
      "position": [
        1300,
        720
      ],
      "id": "d518045d-3962-4254-9358-0b59b8c66747",
      "name": "Auto-fixing Output Parser"
    }
  ],
  "pinData": {
    "OllamaWebhook": [
      {
        "json": {
          "headers": {
            "host": "localhost:5678",
            "user-agent": "curl/8.7.1",
            "accept": "*/*",
            "content-type": "application/json",
            "content-length": "186"
          },
          "params": {},
          "query": {},
          "body": {
            "selectedAgentIds": [
              "data-architect",
              "data-engineer"
            ],
            "selectedStrategy": "collaborative",
            "query": "How to design a scalable data pipeline for real-time analytics?"
          },
          "webhookUrl": "http://localhost:5678/webhook/ollama-de-agent",
          "executionMode": "production"
        }
      }
    ]
  },
  "connections": {
    "Process Agent Request": {
      "main": [
        [
          {
            "node": "Chat Memory Manager",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Basic LLM Chain": {
      "main": [
        [
          {
            "node": "Insert Chat",
            "type": "main",
            "index": 0
          },
          {
            "node": "Respond to Webhook",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Chat Memory Manager": {
      "main": [
        [
          {
            "node": "Aggregate",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Aggregate": {
      "main": [
        [
          {
            "node": "Basic LLM Chain",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Window Buffer Memory": {
      "ai_memory": [
        [
          {
            "node": "Chat Memory Manager",
            "type": "ai_memory",
            "index": 0
          },
          {
            "node": "Chat Trigger",
            "type": "ai_memory",
            "index": 0
          },
          {
            "node": "Insert Chat",
            "type": "ai_memory",
            "index": 0
          }
        ]
      ]
    },
    "Chat Trigger": {
      "main": [
        [
          {
            "node": "Chat Memory Manager",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Ollama Chat Model": {
      "ai_languageModel": [
        [
          {
            "node": "Basic LLM Chain",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    },
    "OllamaWebhook": {
      "main": [
        [
          {
            "node": "Process Agent Request",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Structured Output Parser": {
      "ai_outputParser": [
        [
          {
            "node": "Auto-fixing Output Parser",
            "type": "ai_outputParser",
            "index": 0
          }
        ]
      ]
    },
    "Ollama Chat Model1": {
      "ai_languageModel": [
        [
          {
            "node": "Auto-fixing Output Parser",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    },
    "Auto-fixing Output Parser": {
      "ai_outputParser": [
        [
          {
            "node": "Basic LLM Chain",
            "type": "ai_outputParser",
            "index": 0
          }
        ]
      ]
    }
  },
  "active": true,
  "settings": {
    "executionOrder": "v1"
  },
  "versionId": "b477fcc7-0925-4d20-9919-9ce34fe90378",
  "meta": {
    "templateCredsSetupCompleted": true,
    "instanceId": "6ed7c8c790e9be279bae1f055fa2b85bec988e25017a26c1b6cc75cae7133994"
  },
  "id": "L588GPhJTisyGzk7",
  "tags": [
    {
      "createdAt": "2025-03-23T16:41:12.849Z",
      "updatedAt": "2025-03-23T16:41:12.849Z",
      "id": "pBoE3T7wKTueCALo",
      "name": "ollama"
    },
    {
      "createdAt": "2025-03-23T09:50:13.877Z",
      "updatedAt": "2025-03-23T09:50:13.877Z",
      "id": "uSwVXc72coleYI9d",
      "name": "dataengg"
    }
  ]
}